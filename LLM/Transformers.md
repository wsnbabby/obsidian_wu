> python transformeråº“ç¬”è®°

# æ¦‚å¿µ

- æ ‡é‡
- å‘é‡ï¼ˆä¸€ç»´ï¼ŒäºŒç»´ï¼Œä¸‰ç»´...ï¼‰
- å¼ é‡ (æè¿°å…·æœ‰ä»»æ„æ•°é‡è½´çš„ğ‘›ç»´æ•°ç»„çš„é€šç”¨æ–¹æ³•)

# æœ¯è¯­

- batch_size æ ·æœ¬æ‰¹æ¬¡
- context_len æ–‡æœ¬é•¿åº¦
- d_model å­¦ä¹ ç»´åº¦ï¼ˆè¶…å‚æ•°ï¼‰
- num_heads å¤šå¤´æœºåˆ¶å¤´æ•°ï¼ˆåˆ‡åˆ†d_modelï¼‰

# è®­ç»ƒæµç¨‹

> å¤§æ¨¡å‹å®é™…å°±ä¸¤æ–‡ä»¶
![[Pasted image 20240428104308.png]]

![[Pasted image 20240428135709.png]]

![[Pasted image 20240509093446.png]]

![[Pasted image 20240509093534.png]]
![[Pasted image 20240428105040.png]]

![[Pasted image 20240428113207.png]]
## Tokenization

- åŠ è½½åˆ†è¯å™¨tokenizer.model
```python
# é»˜è®¤åŠ è½½ç›®å½•ä¸‹tokenizer.modelæ–‡ä»¶
tokenizer = AutoTokenizer.from_pretrained("../finetune_demo/model", trust_remote_code=True)
```

- Encoding
```python
# åˆ†è¯tokenization
tokenizer.tokenize("Using a Transformer network is simple")
# ['â–Using', 'â–a', 'â–Trans', 'former', 'â–network', 'â–is', 'â–simple']

# convert_tokens_to_ids
tokenizer("Using a Transformer network is simple")
# {'input_ids': [64790, 64792, 6316, 260, 3107, 17639, 2804, 323, 2718], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'position_ids': [0, 1, 2, 3, 4, 5, 6, 7, 8]}
```

- Decoding
```python
tokenizer.decode([64790, 64792, 6316, 260, 3107, 17639, 2804, 323, 2718])
# [gMASK] sop Using a Transformer network is simple
```

- éšæœºåˆå§‹åŒ–tokenè¯æ±‡è¡¨ç»´åº¦å‘é‡ï¼ˆ`Token Embedding Look-up Table`ï¼‰
![[Pasted image 20240429161121.png]]

## Word Embedding

4æ®µæ–‡æœ¬(batch_size=4ï¼Œå‡è®¾ctx_len=16)
1. `hello world`
2. `today is monday`
3. `Attention Is All You Need`
4. `å¤§æ¨¡å‹ä»¤äººå¤´ç§ƒ`

4æ®µæ ·æœ¬è½¬æ¢æˆtokenç´¢å¼•
![[Pasted image 20240429161208.png]]

æ¯ä¸ªæ ·æœ¬è½¬æ¢æˆç»´åº¦å‘é‡
![[Pasted image 20240429161604.png]]

## Positional Encoding

$$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{\mathrm{model}}})$$
$$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{\mathrm{model}}})$$
```python
for pos in range(16):  
    line = []  
    for i in range(32):  
        line.append(sin(pos / 10000 ** (i * 2 / 64)))  
        line.append(cos(pos / 10000 ** (i * 2 / 64)))  
    print(line)
```

![[Pasted image 20240429160320.png]]


## Attention - QKV

![[Pasted image 20240428144752.png]]

![[Pasted image 20240428145116.png]]

## Layer Normalization

> å±‚å½’ä¸€åŒ–è§„åˆ™:
> 1. å‡å€¼ä¸º0
> 2. æ–¹å·®ä¸º1

$$y=\frac{\alpha-\mu}{\sqrt{\sigma^{2}+\epsilon}}\odot\gamma+\beta $$
ä¸¤ä¸ªå¯è°ƒå‚æ•°ï¼šç³»æ•°Î³ã€åç½®é¡¹Î²

```python
// biaså¯¹åº”ä¸Šé¢åç½®é¡¹Î²
nn.LayerNorm(bias=True)
```

## softmax

>åœ¨æœºå™¨å­¦ä¹ å°¤å…¶æ˜¯æ·±åº¦å­¦ä¹ ä¸­ï¼Œsoftmaxæ˜¯ä¸ªéå¸¸å¸¸ç”¨è€Œä¸”æ¯”è¾ƒé‡è¦çš„å‡½æ•°ï¼Œå°¤å…¶åœ¨å¤šåˆ†ç±»çš„åœºæ™¯ä¸­ä½¿ç”¨å¹¿æ³›ã€‚ä»–æŠŠä¸€äº›è¾“å…¥æ˜ å°„ä¸º0-1ä¹‹é—´çš„å®æ•°ï¼Œå¹¶ä¸”å½’ä¸€åŒ–ä¿è¯å’Œä¸º1ï¼Œå› æ­¤å¤šåˆ†ç±»çš„æ¦‚ç‡ä¹‹å’Œä¹Ÿåˆšå¥½ä¸º1ã€‚

$$\sigma(\overrightarrow{z})_{i}=\frac{e^{z_{i}}}{\sum_{j=1}^{k}e^{z_j}}$$
```python
torch.softmax()
```

![[Pasted image 20240428142844.png]]


## äº¤å‰ç†µæŸå¤±å‡½æ•°

# æ¨ç†åŸç†

![[Pasted image 20240428161257.png]]














# é™æ€é…ç½®

```python 
TOKENIZER_CONFIG_FILE = "tokenizer_config.json"
```

# ä»£ç 

## AutoTokenizer

### from_pretrained
1. æ ¹æ®é…ç½®åŠ è½½`get_tokenizer_config`



## AutoModelForCausalLM

```python
from_pretrained
	
```