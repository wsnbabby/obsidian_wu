
LLM（Large language models）

# 神经网络

### RNN

### CNN

### LSTM


# 模型

### ELMo
### BERT

### GPT



# 训练&微调

### RLHF

### SFT（Supervised Fine-Tuning）
>监督微调



# 公式

### 损失函数

$$loss=\frac{1}{2}(y-\hat{y})^2 $$

### 梯度下降计算（g为导数，lr为学习率）：$$w = w^\prime-lr\cdot g$$


```python
loss = nn.CrossEntropyLoss() # 交叉熵损失函数
loss.backward() # 求导
optimizer.step() # 更新参数
```